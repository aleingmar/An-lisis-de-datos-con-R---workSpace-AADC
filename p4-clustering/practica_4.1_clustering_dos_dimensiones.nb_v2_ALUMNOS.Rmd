---
title: "Clustering: parte I"
output: html_notebook
---

Los algoritmos de *clustering* - en español algoritmos de agrupamiento - construyen grupos de instancias con un mismo comportamiento. Se pueden ver, por lo tanto, como algoritmos que buscan patrones dentro de los datos que estudiamos. Tratan de buscar ejemplos del conjunto de datos con un mismo comportamiento. Ya hemos visto que los dos principales algoritmos son el k-means y los algoritmos de clustering jerárquico.

Veamos un ejemplo en el que trabajamos con puntos en el plano. De esta forma, trabajando con un ejemplo pequeño en tan sólo dos dimensiones, comprenderemos mejor el funcionamiento de estos algoritmos.

Trabajaremos con un fichero de nombre "toy_example.txt" que representa puntos en el plano (dos dimensiones).

```{r}

# Carga el fichero:
puntos<-read.delim("data/toy_example.txt",
                   sep = "\t", 
                   head = FALSE)

# Visualiza su disposión en el plano
plot(puntos) # Los representamos geometricamente

```

Explora brevemente el conjunto de datos:

-   ¿cómo es el *dataframe*?

```{r}

# Función head: 

head(puntos)
```

-   estructura, para comprobar que son datos numéricos.

```{r}

# Función str

str(puntos)

```

-   dimensión, para ver el número de puntos.

```{r}

# Función dim
dim(puntos)


```

### Algoritmo k-Means

El algoritmo k-Means se basa en la elección de un número de dado de centros de masa - de ahí el parámetro 'k' - y la proximidad, según una determinada distancia, de los puntos a cada uno de ellos. Cada centro de masa se recalcula, de manera iterativa, y cuando el proceso es estable se obtienen, por proximidad, los clústeres o grupos deseados.

Como ya comentamos, este algoritmo es muy rápido pero tiene como principal inconveniente el saber cuál es el mejor valor para el parámetro k.

Utiliza la función **kmeans** y aplica una vez el algoritmo con tres centros de masa. Valor de k=3. Nota: consulta los parámetros de entrada *help(kmeans)*. ¿Cuántos parámetros son obligatorios al hacer la llamada? ¿Cuál es el valor por defecto del parámetro *nstar*?

```{r}

# Aplicamos el algoritmo k-means con tres centros de masa y el parámetro nstar igual a 20
km_puntos <- kmeans(puntos, centers=3, nstart=20)
km_puntos
#n strat es la aleatoriedad del algorti

# los vectores de abajo será el punto numero 1 cae en el claster 2

```

Inspeccionamos el resultado que hemos guardado, al hacer la llamada en una variable,

-   ¿Qué devuelve la función *summary*?

```{r}

summary(km_puntos)
#hay 300 puntos
```

-   ¿Qué se obtiene al invocar "..\$cluster" sobre la variable?

```{r}

km_puntos$cluster
```

-   ¿Y si imprime simplemente la variable?

```{r}

print(km_puntos)

```

Visualicemos los resultados mediante la función plot pero pasando como parámetro para el color los clústeres obtenidos.

```{r}


plot(puntos, col=km_puntos$cluster, main= 'Se obtienen 3 puntos') #se obtienen tres clusters
```

### Algoritmo k-Means: control de la aleatoriedad

¿Para que sirve la condicion nstar=20?

Si has leído con detenimiento la ayuda habrás comprobado que el valor de este parámetro sirve para controlar la aleatoriedad, inherente al algoritmo k-Means.

Ejecutemos el algoritmo seis veces con el valor de *nstar=1* y, una vez visto los resultados, repitamos el experimento con *nstar=20*.

-   *nstar=1*

```{r}

par(mfrow=c(2,3)) # construyo una rejilla de 2x3

# ejecutamos 6 veces el algoritmo

for(i in 1:6){
  # nstar
  km_puntos_aux<- kmeans(puntos, centers=3, nstar=1)
  
  
  
  plot(puntos, col= km_puntos_aux$cluster, main= km_puntos_aux$tot.withinss)
  
}




```

-   *nstar=20*

```{r}

par(mfrow=c(2,3)) # construyo una rejilla (para que me salgan 6 graficas 2f y 3 col) de 2x3

# ejecutamos 6 veces el algoritmo

for(i in 1:6){
  # nstar
  km_puntos_aux<- kmeans(puntos, centers=3, nstar=20) # subiendo el valor de nstar cointrola mas la aleatoriedd de los cluster
  # por lo tanto siempre que hagamos kmeans hacerlo asi
  
  
  plot(puntos, col= km_puntos_aux$cluster, main= km_puntos_aux$tot.withinss)
  
}

```

Estudio en la ayuda para qué sirven los parámetros: + nstar + nstar

### Algoritmo k-Means: ¿cómo elegir el valor de k?

Mira en la ayuda el significado *tot.withinss* o valor de compactación. Realizaremos varias ejecuciones del algoritmo variando el valor de k y veremos cómo afecta al valor de compactación.

-   Ejecutamos el algoritmo variando el valor de k de manera incremental: de 1 a 15
-   Guardamos el valor de compactación en cada caso: los almacenamos en un vector
-   Representamos gráficamente esos valores

Pegunta: ¿para qué valor de k se produce un cambio significativo?

(En este caso el valor es k=2)

```{r}

# quiero hacer 15 ejecucuiones variando el numero d egrupos para comparar el valor de compactacion

vector_compactacion <- 0
for (i in 1:15){
  km_puntos_aux2 <- kmeans(puntos, centers=i, nstar=20)
  vector_compactacion[i] <- km_puntos_aux2$tot.withinss

  
}

# hacemos la graficas

par(mfrow= c(1,1))



plot(1:15, vector_compactacion, type= 'b', xlab= 'Numero de clusters',
     ylab= 'valor de compactacion')




```

### Clustering jerárquico

Los algoritmos de clustering jerárquico se basan en la construcción de un dendograma que se "corta" de una determinada manera. En función de cómo se construya este dendograma, y cómo se realice el corte, obtendremos un resultado u otro. Consulta la función **hclust**

Primero construimos el dendograma para lo cual:

-   Se construye la matriz de distancias. ¿Qué distancia toma por defecto
-   Se aplica la función hclust

```{r}

#matriz de distancias
matriz_distancias <- dist(puntos)

#se construye el dendograma
hclust_aux <- hclust(matriz_distancias)

summary(hclust_aux)

#



```

Visualicemos el dendograma.

```{r}
# si corto a la altura 8 tendria dos cluster

plot (hclust_aux, hang = -2)
```

Y en función de dicho dendograma, construimos los clústeres. Consulta la función **hclust**

```{r}
# se puede costar según la altura del endograma
cutree (hclust_aux, h = 7)

# se puede cortar por el numero de cluster que quiero
cutree (hclust_aux, , k=3)

```

Estudiemos las distintas formas de construir el dendograma, en función de cómo definimos la idea de distancia de un punto a conjunto.

```{r}
#LINKAGE COMPLETO
hclust_complete1 <- hclust(dist(puntos), method= "complete")
#plot(hclust_complete1, hang=-2) # para que no salga muy pegado


#LINKAGE MEDIA
hclust_average2 <- hclust(dist(puntos), method= "average")
plot(hclust_average2, hang=-2) # para que no salga muy pegado

# linkage simple --> 

```

Por finalizar, veamos algunas consideraciones adicionales con este ejemplo.

-   El problema del escalado

```{r}


```

-   Comparacion entre el k-Means y el Hierarchical clustering

```{r}
# comparar resultados

corte_hclust_aux <- cutree(hclust_aux, k=3) #3 clusters

table(km_puntos$cluster, corte_hclust_aux)

#INTERPRETACION
#El cluster 3 de kmeans tiene todos los puntos (150) en el cluster 3 del jerarquico 
```
